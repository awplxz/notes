
# 容器集群工作规划

## 集群部署与运营
1. 简化集群部署 （P1)
  - 去掉API server nginx proxy依赖，或者将nginx proxy移到systemd。
  - 支持不部署容器网络，或者支持更简单的容器网络部署（不需要依赖于网络组进行操作）。
  - 去掉RAID1
2. ETCD安全访问 (P2)
3. 集群分区域升级策略（包括k8s, docker，OS升级)，升级时已经运行的container不受影响
4. 访问各集群的堡垒机配置（现在使用Master节点，不是正确的方式）（P0）
5. 测试、部署和升级流程和自动化
目标：建立一整套流程和工具，使得集群测试、升级和部署工作能够安全、流畅的进行，尽量避免人为操作。对于部署和升级工作，工具应该对结果进行校验，保证当前状态跟目标状态一致。
6. 自动化管理、dashboard
目标：提供统一的控制台界面，对各集群的资源和状态进行管理、查询和展示。
7. kube-on-kube自动化运维


## 调度器
1. 调度器性能提升 (P1)
2. 重建POD缺省调度到之前的Node上，在设置一个参数后强制调度到其他Node上。(P2)
3. 支持并行调度：并行调度
4. 支持priority和preemption。（P2)
5. 支持针对大数据业务优化的调度策略: 如binpack策略
6. batch scheduling：
 - https://medium.com/palantir/spark-scheduling-in-kubernetes-4976333235f3 以scheduler extender的方式实现
 - https://volcano.sh/docs/architecture/ 以单独的进程volcano controller和scheduler实现，Job包括多个task，每个task可以有多个replica，如tf的ps/worker，spark的driver/executor。
7. rescheduling
8. descheduling
9. 动态Namespace quota
10. 混合部署策略
  - 标签 + taints
  - pod profile

## Controller
1. DaemonSet：对于某些PoD修改，如toleration，做到不需要重启容器。（P2)
2. StatefulSet：对于Container的某些修改，做到PoD原地重启容器。（P1)
3. StatefulSet/DaemonSet：支持强制PoD里的某些container原地rolling restart，即使container没有修改。（P1)
4. DaemonSet：支持分区域快速重启。（P2)

## Kubelet
1. 支持动态主机网络端口分配。（P2)
2. 实现动态主机端口注册。（P2)
3. 更好支持NUMA调度。（P3)
4. 更好支持priority和preemption。（P1)
5. 支持CPU L3 cache隔离。（P3)
6. 支持lxcfs。（P3)
7. 支持网络隔离。（P3)
8. 去掉kubelet和kube-proxy对本地nginx的依赖。（P1)
9. 确保kubelet重启重新创建Pod时能够成功：现在观察到有些关键pod，例如kube-proxy, filebeat由于configmap无法挂载而失败，需要重新创建pod。(P0)
10. 支持动态调整容器资源。（P3)
11. 通过DevicePlugin支持本地磁盘接入。(P1)
12. 保证节点更新时container不重启。 (P1)

## 隔离
### 机器隔离
- dedicated=namespace：分配给某个namespace使用的机器。只有来自相应namespace的应用才能tolerate这个taint。同时在调度上，tolerate dedicated taint的pod应该require调度到这些taint对应的机器上。
- 特殊机器类型taint：gpu, legacy。
- node标签过多需要有效管理

### 存储隔离
- 用作LocalPV的节点跟Shuffle/HDFS节点物理分开。同时，用作Local PV的device mount到/mnt路径下，并且从/data{0-N}路径下unmount。
- Pod本地持久日志挂载到/data下。
- 非只读HostPath只能挂载/var/log, /data下的路径。
- /data路径下IO和volume size隔离。
- 尽量做到root文件系统只读挂载。
- 物理磁盘分配
  - 可以运行Shuffle/HDFS任务的节点
  - 规划给Kafka的节点
  - 规划给HBase的节点

### 用户隔离
- 使用非root用户运行容器

### 性能隔离
- page cache, slab隔离
- CPU物理核隔离，CPU accounting周期（现在缺省100ms)

### 网路隔离
- HostNetwork的使用
- 容器网络的部署
- 网络策略

## 客户端
1. 维护和更新Java/Go版本的标准客户端 (P1)

## Calico
1. 梳理BIRD BGP peering的配置及相应的网络端配置，在BIRD重启时避免网络抖动。（P0）
2. 将BIRD从calico-node容器中抽离出来，尽量避免BIRD被重启。（P2)
3. 实现一个Node内容器IP分配stickyness及IP重新分配时的时间间隔保证。（P1)
4. 提升容器网络的监控。（P1）
5. 支持不部署容器网络。（P0)

## 日志
1. 完成local file agent方案和部署。（P1)
2. 确保日志采集Pipeline成功运行。（P1)
3. 统一大数据平台的日志采集方案。（P3)

## Admission controller
1. 自动设置toleration及node affinity。（P1）
2. 自动设置ServiceAccount。（P2)
3. 自动设置对容器网络的需求。(P2)
4. 自动设置对GPU的需求。（P2)
5. 检查HostNetwork端口使用。（P1）

## Authentication
1. 提升集群接入BDP的能力。（P0)
2. 集群cert分发和管理方案。（P1)

## 整体规划
1. 完善Local volume的规划。（P0）
2. 容器集群部署和使用Best Practice。（P0)
3. 集群namespace及quota配置与管理。（P1)
4. 集群扩容机制。（P1)
5. 多集群部署和federation方案。（P1)
6. 集群混合部署及隔离方案。（P1）

## 镜像
1. 分布式镜像服务。（P1）
2. 镜像安全。（P1）
3. 共享基础镜像。（P0）
4. 镜像打包服务。（P1）

## 容器引擎
1. cri-o/cri-containerd调研。（P3)
2. docker技术跟进和提升。（P1)
3. Rich container支持。（P3)
4. 使用Overlayfs2取代DeviceMapper。（P1）

## OS
1. 操作系统升级和维护  （P1）
2. kernel升级 (CentOS7.4， P1)
3. 跟进和解决OS和kernel的各项bug。（P1）
4. 对程序代码内存进行lock。（P3）
5. 支持内核hot patch。（P3）

## 集群测试
1. 大规模集群性能测试。（P0）

## 集群运维支持
1. 完善和提升集群监控和设置。（P0）
2. 完善集群上线流程。（P0）
3. 提升预发布环境测试效果。（P0）
4. 提升集群的可调式性，可操作性和可管理性。（P1)
5. 提升运维人员对集群的认识和操作能力。（P1)
6. 完善集群灰度上线的能力和流程。（P1)
7. 集群升级流程。（P1)
