# K8s自主开发的功能列表

## 调度器性能

- k8s调度器的特点：

  - 支持多种调度策略可配置；

  - 方便扩展自定义调度策略；

  - 调度过程分预选和优选（优选过程map-reduce计算方式，map阶段的并行化提升了调度性能）；

  - 支持抢占和优先级调度；

  - 反调度器；

  - 驱逐机制与调度机制相互协作；

  - 无状态pod和有状态pod的调度不一样。

  - 【欠缺gang scheduling】

  - v1.15之后使用调度框架，提供了设计良好的基于插件的接口。

  - k8s调度器属于“共享状态调度器”（还有集中式调度器、双层调度器(yarn+spark)）：共享状态调度架构为了提供高可用性和可扩展性，将除共享状态之外的功能剥离出来成为独立的服务。 状态包括两类：

    - 系统中资源分配和使用的状态

    - 系统中任务调度和执行的状态
    - 参考：https://io-meter.com/2018/02/09/A-summary-of-designing-schedulers/

- 中断all predicates检查：在检查一个node是否适合调度当前pod时，原始方式是把所有predicates都检查一遍；本修改添加一个选项用于中断所有predicates检查，提升检查单个节点的性能。【美团云】

- 提前执行“NodeSelectorTerm转换成requiredNodeSelector”：原来方式是，检查pod nodeAffinity时，每次检查一个node都要执行该转换；修改方式是，将该转换放到获取pod metadata时(向apiserver获取一次)，仅执行一次。同样的修改也针对“PreferredSchedulingTerm转换成preferredNodeSelector“。

- 采样调度：从集群所有node中选择一批node，用来检查当前pod是否适合调度到这些node上；如果过滤出来的node数量未达到阈值，则继续选择下一批node。选择的一批node的数量是： (1)阈值的3倍 (2)集群node总数的1/10，二者之中的最大值。

- 多调度器：

  -  我们需要加上一个模块“Scheduler Controller”, 它存有所有的scheduler实例的信息，包括，type,policy,name以及podQueue或者priorityQueue中等待调度pod的数量。首先，根据pod上的“Type” Annotation选出该类型的所有scheduler实例,然后根据“Policy” Annotition 选出携带有指定Policy的实例集，接着根据每个实例的Queue负载，选出负载较低的实例，最后在该Pod上更新“Name” Annotation为Assigned Scheduler实例的名字。
  -  完成Conflict Detection的工作，因为在多实例调度的时候会出现多个Pod同时调度到同一个Node的情况，目前k8s默认会通过Kubelet来Re-run General Predicates进行本地冲突检测，但是这样的latency会比较高，所以要有一种机制在APIServer中进行Pre Conflict Detection。


## 调度器功能

- 反调度器descheduler：**移除决策不再正确**的调度，降低系统中的熵，让调度器根据当前的状态重新决策。
- SelectorBinpackPriority：SelectorSpreadPriority是将同一个任务的不同pod分散到不同的zone上，不适合storm的场景，因为它增加了pod之间网络通信的成本；binpack策略是将同一个任务的不同pod尽量调度到相同的zone上。
- SelectorSpreadPriority优化：待调度pod的兄弟pod分布在不同的node上，该策略先计算每个node上的兄弟pod数量，然后按zone对node作聚合，如果兄弟pod数越少则zone的分数越高。这将导致具有较少node的zone的分数较高。例如：zone-A有1个node，其上有1个兄弟pod；zone-B有5个node，每个node上各有1个兄弟pod；那么两个zone上的pod数量分别是1和5，分数分别是`10*(4/5)=8`，`10*(0/5)=0`，那么待调度pod会优先调度到zone-A的node上。这将导致很多pod扎堆调度到zone-A的一个node上，使该node负载过高。修改方式是，在计算zone分数时，把node数量考虑进去，计算`pod数量/node数量`，即将`平均每个node上的pod数量`作为分数。
- 禁止”使用了容器网络的pod调度到只有主机网络的node上“：主机网络node具有标签node.bcc.jd.com/network=host；如果pod使用了容器网络，则不能与主机网络节点匹配成功。
- 考虑node的io/memory实时使用情况作为调度策略：从prometheus拉取所有node的io/memory指标分数，缓存在scheduler cache中。
- pod使用特定的调度策略：调度策略的配置中，指定某个策略只对哪个业务(key)启用；业务pod使用annotation指定它是哪个业务(key)；策略配置中的key可以是`"ns:<namespace>"`的形式，表示对特定的namespace中的所有pod生效。
- 打印node分数：优选策略执行时，打印出pod对应的priority分数最高的几个node。
- https://git.jd.com/bag/kubernetes/merge_requests/119/diffs

## 其他组件功能

- 【controller-mamager】优先回收”成功退出的pod“：podGC时，原先的方式没有区分成功与失败的哪个先删除，修改方式是先删除退出状态为成功的pod，因为失败的pod更具有保留价值。

- 【controlller-manager】双路条件判断node notready：原来的方式是controller-manager从apiserver上获取node状态，如果node状态正常但是kubelet上报node状态到apiserver失败，则node也会被认为notready，从而导致驱逐node；修改方式是controller-manager也从prometheus上获取node状态，两条网络路径作为判断标准。改进：有的时候kubelet无法上报node状态的时间过长，这时也应该驱逐node(例如nginx proxy宕掉)，改进方式是，当node状态为unknown的时间超过一个阈值时，不考虑prometheus的监控结果。

- 【kubelet】kubelet重启会检查已有pod的node affinity是否满足现有的node，如不满足会驱逐相应pod。这种行为导致了kubelet重启与业务pod之前的耦合，有可能导致业务意外的失败。修改方式是，kubelet重启时，对于正在运行的pod，只作必要检查(如资源、端口、容器网络)，不检查node selector/affinity。

- 【apiserver】添加admission controller plugin，检查pod使用的主机端口是否在指定的范围内。

- 【apiserver】添加admission controller plugin，给特定业务的pod打上affinity和taint。解决的问题：有些node只提供给特定的业务，node上有label和taint允许该业务使用；如果用户忘了给业务pod打上affinity和taint就会调度失败。


## k8s特点

  - 工业级的容器编排平台 
  - 声明式API
  - 面向应用(服务) 
  - 自动化管理:部署、修复、伸缩
  - 可扩展:network、device、storage 
  - 多种部署方式:主机、云 
  - 支持多种workload

